{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install tqdm\n",
    "#!pip install evaluate\n",
    "#!pip install torch\n",
    "#!pip install accelerate\n",
    "#!pip install numpy\n",
    "#!pip install matplotlib\n",
    "#!pip install tensorboardx\n",
    "#!pip install scikit-learn\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\") \n",
    "\n",
    "# Load the SQuAD dataset\n",
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e49a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace original teacher model initialization\n",
    "teacher_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    teacher_model_name,\n",
    "    trust_remote_code=True  # Required for DeepSeek models\n",
    ")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    teacher_model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False  # Recommended for DeepSeek models\n",
    ")\n",
    "\n",
    "# Modify preprocessing for DeepSeek's tokenization\n",
    "def preprocess_teacher_train(example):\n",
    "    inputs = teacher_tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=512,  # Matches DeepSeek's context window\n",
    "        stride=96,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True  # Explicitly enable special tokens\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed974e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should do a training pass on SQuAD1.1 to initialise the weights in the Teacher model.\n",
    "# This will also stop the warning from displaying\n",
    "\n",
    "# New function to preprocess training data for teacher\n",
    "def preprocess_teacher_train(example):\n",
    "    # roberta-base-squad-v1\n",
    "    #inputs = teacher_tokenizer(\n",
    "    #    example[\"question\"],\n",
    "    #    example[\"context\"],\n",
    "    #    truncation=True,\n",
    "    #    max_length=384,\n",
    "    #    stride=128,\n",
    "    #    return_overflowing_tokens=True,\n",
    "    #    return_offsets_mapping=True,\n",
    "    #    padding=\"max_length\"\n",
    "    #)\n",
    "    \n",
    "    # DeepSeek-R1-Distill-Qwen-1.5B\n",
    "    inputs = teacher_tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        stride=96,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True  # Explicitly enable special tokens\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = example[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Handle empty sequence_ids case\n",
    "        if not sequence_ids:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        # Find context start with boundary checks\n",
    "        idx = 0\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx if idx < len(sequence_ids) else 0\n",
    "\n",
    "        # Find context end with boundary checks\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1 if idx > 0 else 0\n",
    "\n",
    "        # Handle answer position calculation\n",
    "        if (context_start >= len(offset) or \n",
    "            context_end >= len(offset) or\n",
    "            offset[context_start][0] > end_char or \n",
    "            offset[context_end][1] < start_char):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Find start position with bounds checking\n",
    "            idx = context_start\n",
    "            while idx <= context_end and idx < len(offset) and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(min(idx - 1, len(offset)-1))\n",
    "            \n",
    "            # Find end position with bounds checking\n",
    "            idx = context_end\n",
    "            while idx >= context_start and idx < len(offset) and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(min(idx + 1, len(offset)-1))\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "# Teacher Training Arguments\n",
    "teacher_training_args = TrainingArguments(\n",
    "    output_dir=\"./teacher_train\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,  # Increased logging frequency\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    learning_rate=1e-5,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "# Custom progress callback\n",
    "class TeacherTrainingProgress(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"üöÄ Starting training with {args.num_train_epochs} epochs\")\n",
    "        print(f\"üìä Batch size: {args.per_device_train_batch_size}\")\n",
    "        print(f\"üîç Evaluation every {args.eval_steps} steps\")\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"\\n‚è≥ Starting epoch {state.epoch}/{args.num_train_epochs}\")\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            print(f\"Step {state.global_step}: Loss {logs['loss']:.4f}\")\n",
    "        if logs and 'eval_loss' in logs:\n",
    "            print(f\"Validation Loss: {logs['eval_loss']:.4f}\")\n",
    "            print(f\"Exact Match: {logs['eval_exact_match']:.2f}%\")\n",
    "            print(f\"F1 Score: {logs['eval_f1']:.2f}%\")\n",
    "\n",
    "# Add metrics computation to Trainer\n",
    "def compute_metrics(p):\n",
    "\n",
    "    # Convert logits to predictions\n",
    "    start_pred = np.argmax(p.predictions[0], axis=1)\n",
    "    end_pred = np.argmax(p.predictions[1], axis=1)\n",
    "    \n",
    "    # Get true positions\n",
    "    start_true = p.label_ids[0]\n",
    "    end_true = p.label_ids[1]\n",
    "    \n",
    "    # Calculate exact match\n",
    "    exact_matches = np.logical_and(\n",
    "        start_pred == start_true,\n",
    "        end_pred == end_true\n",
    "    )\n",
    "\n",
    "    # Calculate span F1\n",
    "    def overlap_f1(p_start, p_end, t_start, t_end):\n",
    "        pred_span = set(range(p_start, p_end+1))\n",
    "        true_span = set(range(t_start, t_end+1))\n",
    "        overlap = len(pred_span & true_span)\n",
    "        precision = overlap / len(pred_span) if pred_span else 0\n",
    "        recall = overlap / len(true_span) if true_span else 0\n",
    "        return 2*(precision*recall)/(precision+recall) if (precision+recall) else 0\n",
    "    \n",
    "    f1_scores = [\n",
    "        overlap_f1(sp, ep, st, et)\n",
    "        for sp, ep, st, et in zip(start_pred, end_pred, start_true, end_true)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": np.mean(exact_matches) * 100,\n",
    "        \"f1\": np.mean(f1_scores) * 100\n",
    "    }\n",
    "\n",
    "\n",
    "# Create Trainer for teacher\n",
    "teacher_trainer = Trainer(\n",
    "    model=teacher_model,\n",
    "    args=teacher_training_args,\n",
    "    train_dataset=squad[\"train\"].map(preprocess_teacher_train, batched=True, remove_columns=squad[\"train\"].column_names),\n",
    "    eval_dataset=squad[\"validation\"].map(preprocess_teacher_train, batched=True, remove_columns=squad[\"validation\"].column_names),\n",
    "    tokenizer=teacher_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[TeacherTrainingProgress()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train teacher model\n",
    "print(\"\\nTraining Teacher Model on SQuAD1.1...\")\n",
    "teacher_trainer.train()\n",
    "teacher_model.save_pretrained(\"./DeepSeek-R1-Distill-Qwen-1.5B-trained\")\n",
    "teacher_tokenizer.save_pretrained(\"./DeepSeek-R1-Distill-Qwen-1.5B-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nRe-loading optimized teacher model\")\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(\"./trained_teacher\")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(\"./trained_teacher\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRML_Assign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
