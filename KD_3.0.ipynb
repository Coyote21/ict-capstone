{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9a7d3-2b0c-4899-81ea-b680565c7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Initial Setup**\n",
    "\n",
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install tqdm\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "#from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "loader_batch_size = 4\n",
    "\n",
    "# Set calculation device as either \"cuda\" (GPU) or \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098eeea-e09b-4084-b788-2612e9d7644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Load Models**\n",
    "# Load Teacher and Student Models\n",
    "#teacher_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "#teacher_model_name = \"Jesujuwon/distilgpt2-squad\" #82M\n",
    "teacher_model_name = \"mrm8488/electra-base-finetuned-squadv1\"\n",
    "#student_model_name = \"Locutusque/TinyMistral-248M\"\n",
    "student_model_name = \"tniranjan/finetuned_tinystories_33M_pretrained_tinystories_ta\"\n",
    "\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name)\n",
    "student_model = AutoModelForCausalLM.from_pretrained(student_model_name)\n",
    "\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "\n",
    "# Add padding tokens if missing and reconfigure models\n",
    "for tokenizer, model in [(teacher_tokenizer, teacher_model), (student_tokenizer, student_model)]:\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26acfe-77cd-4bae-890c-bac4a9285cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Dataset Processing**\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_batch(batch, tokenizer, max_length=256):\n",
    "    # Extract questions and contexts from the batch\n",
    "    questions = [example[\"question\"] for example in batch]\n",
    "    contexts = [example[\"context\"] for example in batch]\n",
    "    \n",
    "    # Tokenize context and question\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Extract answer text (use the first answer for simplicity)\n",
    "    answer_texts = [\n",
    "        example[\"answers\"][\"text\"][0] if len(example[\"answers\"][\"text\"]) > 0 else \"\" \n",
    "        for example in batch\n",
    "    ]\n",
    "    \n",
    "    # Tokenize answers\n",
    "    labels = tokenizer(\n",
    "        answer_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "    # Add labels to inputs\n",
    "    inputs[\"labels\"] = labels\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f81ad5-3433-4d68-bf61-3e808a89f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Load Dataset**\n",
    "\n",
    "# Load SQuAD1.1 Dataset\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'validation': 'plain_text/validation-00000-of-00001.parquet'}\n",
    "train_df = pd.read_parquet(\"hf://datasets/rajpurkar/squad/\" + splits[\"train\"])\n",
    "validation_df = pd.read_parquet(\"hf://datasets/rajpurkar/squad/\" + splits[\"validation\"])\n",
    "\n",
    "# **Process Dataset**\n",
    "\n",
    "# Need to reduce the size of the train dataset to make testing much faster.\n",
    "# We can use the full dataset again once we know the code is working.\n",
    "\n",
    "##### Dataset size reduction code here #####\n",
    "reduced_train_df = train_df.sample(frac=0.01, random_state=42)\n",
    "reduced_validation_df = validation_df.sample(frac=0.01, random_state=42)\n",
    "\n",
    "print(f\"Original train size: {len(train_df)}\")\n",
    "print(f\"Original valid size: {len(validation_df)}\")\n",
    "print(f\"Reduced train size: {len(reduced_train_df)}\")\n",
    "print(f\"Reduced valid size: {len(reduced_validation_df)}\")\n",
    "\n",
    "full_train_df = train_df\n",
    "train_df = reduced_train_df\n",
    "\n",
    "full_validation_df = validation_df\n",
    "validation_df = reduced_validation_df\n",
    "\n",
    "total_batches = math.ceil(len(train_df) / loader_batch_size)\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries for batch processing\n",
    "train_data = train_df.to_dict(orient=\"records\")\n",
    "validation_data = validation_df.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b9a8e-2c83-4776-b23f-13ec6ffacd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch Dataset\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Preprocess data in batches\n",
    "def process_dataset(data, tokenizer):\n",
    "    processed_data = []\n",
    "    batch_size = 32\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        processed_batch = preprocess_batch(batch, tokenizer)\n",
    "        processed_data.append(processed_batch)\n",
    "\n",
    "    # Combine all processed batches into a single dataset\n",
    "    input_ids = torch.cat([batch[\"input_ids\"] for batch in processed_data])\n",
    "    attention_mask = torch.cat([batch[\"attention_mask\"] for batch in processed_data])\n",
    "    labels = torch.cat([batch[\"labels\"] for batch in processed_data])\n",
    "\n",
    "    dataset = QADataset(input_ids, attention_mask, labels)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_dataset = process_dataset(train_data, student_tokenizer)\n",
    "validation_dataset = process_dataset(validation_data, student_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=loader_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=loader_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86220b2-264d-4e73-a6a1-3e90d2641697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Evaluate the pre-distillation performance of both Teacher and Student model and display results **\n",
    "\n",
    "# Load SQuAD metric\n",
    "squad_metric = load(\"squad\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, tokenizer, data, max_length=256):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    print(f\"Evaluating on full validation set: {len(data)} examples\")\n",
    "    \n",
    "    for example in tqdm(data):\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"context\"]\n",
    "        true_answers = example[\"answers\"][\"text\"]\n",
    "\n",
    "        input_text = f\"{question} {tokenizer.sep_token} {context}\"\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],  # Explicit attention mask\n",
    "                max_length=512,\n",
    "                pad_token_id=tokenizer.pad_token_id,  # Explicit pad token\n",
    "                #do_sample=False\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        pred_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append({\"id\": example[\"id\"], \"prediction_text\": pred_answer})\n",
    "        references.append({\"id\": example[\"id\"], \"answers\": example[\"answers\"]})\n",
    "\n",
    "    results = squad_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"Exact Match (EM): {results['exact_match']:.2f}\")\n",
    "    print(f\"F1 Score: {results['f1']:.2f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# ** Evaluate the post-distillation performance of the Student model and display results **\n",
    "print(\"==== Pre-Distillation Evaluation ====\")\n",
    "\n",
    "print(\"\\n🔹 Teacher Model:\")\n",
    "evaluate_model(teacher_model, teacher_tokenizer, validation_data)\n",
    "\n",
    "print(\"\\n🔹 Student Model:\")\n",
    "evaluate_model(student_model, student_tokenizer, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7d55e-0d56-4d10-b867-337f514284f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyAligner:\n",
    "    def __init__(self, teacher_tokenizer, student_tokenizer):\n",
    "        # Create token-to-ID mappings\n",
    "        teacher_vocab = teacher_tokenizer.get_vocab()\n",
    "        student_vocab = student_tokenizer.get_vocab()\n",
    "        \n",
    "        # Build alignment mapping\n",
    "        self.alignment_matrix = defaultdict(lambda: student_tokenizer.unk_token_id)\n",
    "        for token, tid in teacher_vocab.items():\n",
    "            if token in student_vocab:\n",
    "                self.alignment_matrix[tid] = student_vocab[token]\n",
    "                \n",
    "    def project_logits(self, teacher_preds):\n",
    "        # Initialize aligned predictions with unknown token ID\n",
    "        aligned_preds = torch.full_like(teacher_preds, fill_value=self.alignment_matrix.default_factory())\n",
    "        \n",
    "        # Map each teacher token ID to its corresponding student token ID\n",
    "        for tid in range(teacher_preds.max().item() + 1):  # Iterate over all possible teacher token IDs\n",
    "            mask = (teacher_preds == tid)  # Mask where teacher predicts this token ID\n",
    "            aligned_preds[mask] = self.alignment_matrix[tid]  # Map to student token ID\n",
    "        \n",
    "        return aligned_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeadecd-3b46-4cde-9a8d-fb96be9a4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Knowledge Distillation Training Loop**\n",
    "\n",
    "def train_student_with_distillation(teacher_model, student_model, train_loader, epochs=3):\n",
    "   \n",
    "    # Initialize alignment\n",
    "    aligner = VocabularyAligner(teacher_tokenizer, student_tokenizer)\n",
    "    \n",
    "    # Move models to device\n",
    "    teacher_model.to(device)\n",
    "    student_model.to(device)\n",
    "    \n",
    "    # Use gradient checkpointing to save memory\n",
    "    teacher_model.gradient_checkpointing_enable()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Starting epoch {epoch + 1}\")\n",
    "        student_model.train()\n",
    "\n",
    "        batch_count = 1\n",
    "        for batch in train_loader:\n",
    "            print(f\"\\rBatch: {batch_count}/{total_batches}\", end=\"\", flush=True)\n",
    "            inputs_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get teacher predictions (no full logits)\n",
    "                teacher_outputs = teacher_model(input_ids=inputs_ids, attention_mask=attention_mask)\n",
    "                teacher_preds = torch.argmax(teacher_outputs.logits, dim=-1)\n",
    "                \n",
    "            # Project teacher predictions to student vocab\n",
    "            aligned_teacher_preds = aligner.project_logits(teacher_preds).to(device)\n",
    "            \n",
    "            # Student forward pass\n",
    "            student_outputs = student_model(input_ids=inputs_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            \n",
    "            loss = nn.CrossEntropyLoss()(\n",
    "                student_outputs.logits.view(-1, student_model.config.vocab_size),  # Flatten logits\n",
    "                aligned_teacher_preds.view(-1)  # Flatten targets\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_count += 1\n",
    "    \n",
    "        print(f\"\\nEpoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2500c7d-ceef-449e-86e0-6f7d53b1554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "train_student_with_distillation(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    train_loader,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "#### Results Display Code HERE ####\n",
    "print(\"\\n==== Post-Distillation Evaluation ====\")\n",
    "\n",
    "print(\"\\n🔹 Student Model:\")\n",
    "evaluate_model(student_model, student_tokenizer, validation_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
