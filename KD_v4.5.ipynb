{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b11c4a-1b45-40e1-830f-4149f32bd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install tqdm\n",
    "#!pip install evaluate\n",
    "#!pip install torch\n",
    "#!pip install accelerate\n",
    "#!pip install numpy\n",
    "#!pip install matplotlib\n",
    "#!pip install tensorboardx\n",
    "#!pip install scikit-learn\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments, TrainerCallback\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import torch\n",
    "#import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Suppress specific Hugging Face logging messages\n",
    "#transformers_logger = logging.getLogger(\"transformers\")\n",
    "#transformers_logger.setLevel(logging.ERROR)\n",
    "\n",
    "debugging = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "loss_per_epoch = []        # Loss per Epoch\n",
    "exact_match_before = 0     # Exact Match score before distillation\n",
    "exact_match_after = 0      # Exact Match score after distillation\n",
    "f1_score_before = 0        # F1 score before distillation\n",
    "f1_score_after = 0         # F1 score after distillation\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "squad = load_dataset(\"squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e594dc5-a393-400e-8c63-8617ac13e248",
   "metadata": {},
   "source": [
    "# Load teacher model and tokenizer\n",
    "teacher_model_name = \"csarron/roberta-base-squad-v1\"\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(teacher_model_name)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21eec25c-5079-478d-94a5-c747581dd5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cbb3e3c9c64cfa883c9a98c1c7f5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2889a082a5f415481ae8070efc30c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight', 'transformer.embed_tokens.weight', 'transformer.layers.0.input_layernorm.weight', 'transformer.layers.0.mlp.down_proj.weight', 'transformer.layers.0.mlp.gate_proj.weight', 'transformer.layers.0.mlp.up_proj.weight', 'transformer.layers.0.post_attention_layernorm.weight', 'transformer.layers.0.self_attn.k_proj.bias', 'transformer.layers.0.self_attn.k_proj.weight', 'transformer.layers.0.self_attn.o_proj.weight', 'transformer.layers.0.self_attn.q_proj.bias', 'transformer.layers.0.self_attn.q_proj.weight', 'transformer.layers.0.self_attn.v_proj.bias', 'transformer.layers.0.self_attn.v_proj.weight', 'transformer.layers.1.input_layernorm.weight', 'transformer.layers.1.mlp.down_proj.weight', 'transformer.layers.1.mlp.gate_proj.weight', 'transformer.layers.1.mlp.up_proj.weight', 'transformer.layers.1.post_attention_layernorm.weight', 'transformer.layers.1.self_attn.k_proj.bias', 'transformer.layers.1.self_attn.k_proj.weight', 'transformer.layers.1.self_attn.o_proj.weight', 'transformer.layers.1.self_attn.q_proj.bias', 'transformer.layers.1.self_attn.q_proj.weight', 'transformer.layers.1.self_attn.v_proj.bias', 'transformer.layers.1.self_attn.v_proj.weight', 'transformer.layers.10.input_layernorm.weight', 'transformer.layers.10.mlp.down_proj.weight', 'transformer.layers.10.mlp.gate_proj.weight', 'transformer.layers.10.mlp.up_proj.weight', 'transformer.layers.10.post_attention_layernorm.weight', 'transformer.layers.10.self_attn.k_proj.bias', 'transformer.layers.10.self_attn.k_proj.weight', 'transformer.layers.10.self_attn.o_proj.weight', 'transformer.layers.10.self_attn.q_proj.bias', 'transformer.layers.10.self_attn.q_proj.weight', 'transformer.layers.10.self_attn.v_proj.bias', 'transformer.layers.10.self_attn.v_proj.weight', 'transformer.layers.11.input_layernorm.weight', 'transformer.layers.11.mlp.down_proj.weight', 'transformer.layers.11.mlp.gate_proj.weight', 'transformer.layers.11.mlp.up_proj.weight', 'transformer.layers.11.post_attention_layernorm.weight', 'transformer.layers.11.self_attn.k_proj.bias', 'transformer.layers.11.self_attn.k_proj.weight', 'transformer.layers.11.self_attn.o_proj.weight', 'transformer.layers.11.self_attn.q_proj.bias', 'transformer.layers.11.self_attn.q_proj.weight', 'transformer.layers.11.self_attn.v_proj.bias', 'transformer.layers.11.self_attn.v_proj.weight', 'transformer.layers.12.input_layernorm.weight', 'transformer.layers.12.mlp.down_proj.weight', 'transformer.layers.12.mlp.gate_proj.weight', 'transformer.layers.12.mlp.up_proj.weight', 'transformer.layers.12.post_attention_layernorm.weight', 'transformer.layers.12.self_attn.k_proj.bias', 'transformer.layers.12.self_attn.k_proj.weight', 'transformer.layers.12.self_attn.o_proj.weight', 'transformer.layers.12.self_attn.q_proj.bias', 'transformer.layers.12.self_attn.q_proj.weight', 'transformer.layers.12.self_attn.v_proj.bias', 'transformer.layers.12.self_attn.v_proj.weight', 'transformer.layers.13.input_layernorm.weight', 'transformer.layers.13.mlp.down_proj.weight', 'transformer.layers.13.mlp.gate_proj.weight', 'transformer.layers.13.mlp.up_proj.weight', 'transformer.layers.13.post_attention_layernorm.weight', 'transformer.layers.13.self_attn.k_proj.bias', 'transformer.layers.13.self_attn.k_proj.weight', 'transformer.layers.13.self_attn.o_proj.weight', 'transformer.layers.13.self_attn.q_proj.bias', 'transformer.layers.13.self_attn.q_proj.weight', 'transformer.layers.13.self_attn.v_proj.bias', 'transformer.layers.13.self_attn.v_proj.weight', 'transformer.layers.14.input_layernorm.weight', 'transformer.layers.14.mlp.down_proj.weight', 'transformer.layers.14.mlp.gate_proj.weight', 'transformer.layers.14.mlp.up_proj.weight', 'transformer.layers.14.post_attention_layernorm.weight', 'transformer.layers.14.self_attn.k_proj.bias', 'transformer.layers.14.self_attn.k_proj.weight', 'transformer.layers.14.self_attn.o_proj.weight', 'transformer.layers.14.self_attn.q_proj.bias', 'transformer.layers.14.self_attn.q_proj.weight', 'transformer.layers.14.self_attn.v_proj.bias', 'transformer.layers.14.self_attn.v_proj.weight', 'transformer.layers.15.input_layernorm.weight', 'transformer.layers.15.mlp.down_proj.weight', 'transformer.layers.15.mlp.gate_proj.weight', 'transformer.layers.15.mlp.up_proj.weight', 'transformer.layers.15.post_attention_layernorm.weight', 'transformer.layers.15.self_attn.k_proj.bias', 'transformer.layers.15.self_attn.k_proj.weight', 'transformer.layers.15.self_attn.o_proj.weight', 'transformer.layers.15.self_attn.q_proj.bias', 'transformer.layers.15.self_attn.q_proj.weight', 'transformer.layers.15.self_attn.v_proj.bias', 'transformer.layers.15.self_attn.v_proj.weight', 'transformer.layers.16.input_layernorm.weight', 'transformer.layers.16.mlp.down_proj.weight', 'transformer.layers.16.mlp.gate_proj.weight', 'transformer.layers.16.mlp.up_proj.weight', 'transformer.layers.16.post_attention_layernorm.weight', 'transformer.layers.16.self_attn.k_proj.bias', 'transformer.layers.16.self_attn.k_proj.weight', 'transformer.layers.16.self_attn.o_proj.weight', 'transformer.layers.16.self_attn.q_proj.bias', 'transformer.layers.16.self_attn.q_proj.weight', 'transformer.layers.16.self_attn.v_proj.bias', 'transformer.layers.16.self_attn.v_proj.weight', 'transformer.layers.17.input_layernorm.weight', 'transformer.layers.17.mlp.down_proj.weight', 'transformer.layers.17.mlp.gate_proj.weight', 'transformer.layers.17.mlp.up_proj.weight', 'transformer.layers.17.post_attention_layernorm.weight', 'transformer.layers.17.self_attn.k_proj.bias', 'transformer.layers.17.self_attn.k_proj.weight', 'transformer.layers.17.self_attn.o_proj.weight', 'transformer.layers.17.self_attn.q_proj.bias', 'transformer.layers.17.self_attn.q_proj.weight', 'transformer.layers.17.self_attn.v_proj.bias', 'transformer.layers.17.self_attn.v_proj.weight', 'transformer.layers.18.input_layernorm.weight', 'transformer.layers.18.mlp.down_proj.weight', 'transformer.layers.18.mlp.gate_proj.weight', 'transformer.layers.18.mlp.up_proj.weight', 'transformer.layers.18.post_attention_layernorm.weight', 'transformer.layers.18.self_attn.k_proj.bias', 'transformer.layers.18.self_attn.k_proj.weight', 'transformer.layers.18.self_attn.o_proj.weight', 'transformer.layers.18.self_attn.q_proj.bias', 'transformer.layers.18.self_attn.q_proj.weight', 'transformer.layers.18.self_attn.v_proj.bias', 'transformer.layers.18.self_attn.v_proj.weight', 'transformer.layers.19.input_layernorm.weight', 'transformer.layers.19.mlp.down_proj.weight', 'transformer.layers.19.mlp.gate_proj.weight', 'transformer.layers.19.mlp.up_proj.weight', 'transformer.layers.19.post_attention_layernorm.weight', 'transformer.layers.19.self_attn.k_proj.bias', 'transformer.layers.19.self_attn.k_proj.weight', 'transformer.layers.19.self_attn.o_proj.weight', 'transformer.layers.19.self_attn.q_proj.bias', 'transformer.layers.19.self_attn.q_proj.weight', 'transformer.layers.19.self_attn.v_proj.bias', 'transformer.layers.19.self_attn.v_proj.weight', 'transformer.layers.2.input_layernorm.weight', 'transformer.layers.2.mlp.down_proj.weight', 'transformer.layers.2.mlp.gate_proj.weight', 'transformer.layers.2.mlp.up_proj.weight', 'transformer.layers.2.post_attention_layernorm.weight', 'transformer.layers.2.self_attn.k_proj.bias', 'transformer.layers.2.self_attn.k_proj.weight', 'transformer.layers.2.self_attn.o_proj.weight', 'transformer.layers.2.self_attn.q_proj.bias', 'transformer.layers.2.self_attn.q_proj.weight', 'transformer.layers.2.self_attn.v_proj.bias', 'transformer.layers.2.self_attn.v_proj.weight', 'transformer.layers.20.input_layernorm.weight', 'transformer.layers.20.mlp.down_proj.weight', 'transformer.layers.20.mlp.gate_proj.weight', 'transformer.layers.20.mlp.up_proj.weight', 'transformer.layers.20.post_attention_layernorm.weight', 'transformer.layers.20.self_attn.k_proj.bias', 'transformer.layers.20.self_attn.k_proj.weight', 'transformer.layers.20.self_attn.o_proj.weight', 'transformer.layers.20.self_attn.q_proj.bias', 'transformer.layers.20.self_attn.q_proj.weight', 'transformer.layers.20.self_attn.v_proj.bias', 'transformer.layers.20.self_attn.v_proj.weight', 'transformer.layers.21.input_layernorm.weight', 'transformer.layers.21.mlp.down_proj.weight', 'transformer.layers.21.mlp.gate_proj.weight', 'transformer.layers.21.mlp.up_proj.weight', 'transformer.layers.21.post_attention_layernorm.weight', 'transformer.layers.21.self_attn.k_proj.bias', 'transformer.layers.21.self_attn.k_proj.weight', 'transformer.layers.21.self_attn.o_proj.weight', 'transformer.layers.21.self_attn.q_proj.bias', 'transformer.layers.21.self_attn.q_proj.weight', 'transformer.layers.21.self_attn.v_proj.bias', 'transformer.layers.21.self_attn.v_proj.weight', 'transformer.layers.22.input_layernorm.weight', 'transformer.layers.22.mlp.down_proj.weight', 'transformer.layers.22.mlp.gate_proj.weight', 'transformer.layers.22.mlp.up_proj.weight', 'transformer.layers.22.post_attention_layernorm.weight', 'transformer.layers.22.self_attn.k_proj.bias', 'transformer.layers.22.self_attn.k_proj.weight', 'transformer.layers.22.self_attn.o_proj.weight', 'transformer.layers.22.self_attn.q_proj.bias', 'transformer.layers.22.self_attn.q_proj.weight', 'transformer.layers.22.self_attn.v_proj.bias', 'transformer.layers.22.self_attn.v_proj.weight', 'transformer.layers.23.input_layernorm.weight', 'transformer.layers.23.mlp.down_proj.weight', 'transformer.layers.23.mlp.gate_proj.weight', 'transformer.layers.23.mlp.up_proj.weight', 'transformer.layers.23.post_attention_layernorm.weight', 'transformer.layers.23.self_attn.k_proj.bias', 'transformer.layers.23.self_attn.k_proj.weight', 'transformer.layers.23.self_attn.o_proj.weight', 'transformer.layers.23.self_attn.q_proj.bias', 'transformer.layers.23.self_attn.q_proj.weight', 'transformer.layers.23.self_attn.v_proj.bias', 'transformer.layers.23.self_attn.v_proj.weight', 'transformer.layers.24.input_layernorm.weight', 'transformer.layers.24.mlp.down_proj.weight', 'transformer.layers.24.mlp.gate_proj.weight', 'transformer.layers.24.mlp.up_proj.weight', 'transformer.layers.24.post_attention_layernorm.weight', 'transformer.layers.24.self_attn.k_proj.bias', 'transformer.layers.24.self_attn.k_proj.weight', 'transformer.layers.24.self_attn.o_proj.weight', 'transformer.layers.24.self_attn.q_proj.bias', 'transformer.layers.24.self_attn.q_proj.weight', 'transformer.layers.24.self_attn.v_proj.bias', 'transformer.layers.24.self_attn.v_proj.weight', 'transformer.layers.25.input_layernorm.weight', 'transformer.layers.25.mlp.down_proj.weight', 'transformer.layers.25.mlp.gate_proj.weight', 'transformer.layers.25.mlp.up_proj.weight', 'transformer.layers.25.post_attention_layernorm.weight', 'transformer.layers.25.self_attn.k_proj.bias', 'transformer.layers.25.self_attn.k_proj.weight', 'transformer.layers.25.self_attn.o_proj.weight', 'transformer.layers.25.self_attn.q_proj.bias', 'transformer.layers.25.self_attn.q_proj.weight', 'transformer.layers.25.self_attn.v_proj.bias', 'transformer.layers.25.self_attn.v_proj.weight', 'transformer.layers.26.input_layernorm.weight', 'transformer.layers.26.mlp.down_proj.weight', 'transformer.layers.26.mlp.gate_proj.weight', 'transformer.layers.26.mlp.up_proj.weight', 'transformer.layers.26.post_attention_layernorm.weight', 'transformer.layers.26.self_attn.k_proj.bias', 'transformer.layers.26.self_attn.k_proj.weight', 'transformer.layers.26.self_attn.o_proj.weight', 'transformer.layers.26.self_attn.q_proj.bias', 'transformer.layers.26.self_attn.q_proj.weight', 'transformer.layers.26.self_attn.v_proj.bias', 'transformer.layers.26.self_attn.v_proj.weight', 'transformer.layers.27.input_layernorm.weight', 'transformer.layers.27.mlp.down_proj.weight', 'transformer.layers.27.mlp.gate_proj.weight', 'transformer.layers.27.mlp.up_proj.weight', 'transformer.layers.27.post_attention_layernorm.weight', 'transformer.layers.27.self_attn.k_proj.bias', 'transformer.layers.27.self_attn.k_proj.weight', 'transformer.layers.27.self_attn.o_proj.weight', 'transformer.layers.27.self_attn.q_proj.bias', 'transformer.layers.27.self_attn.q_proj.weight', 'transformer.layers.27.self_attn.v_proj.bias', 'transformer.layers.27.self_attn.v_proj.weight', 'transformer.layers.3.input_layernorm.weight', 'transformer.layers.3.mlp.down_proj.weight', 'transformer.layers.3.mlp.gate_proj.weight', 'transformer.layers.3.mlp.up_proj.weight', 'transformer.layers.3.post_attention_layernorm.weight', 'transformer.layers.3.self_attn.k_proj.bias', 'transformer.layers.3.self_attn.k_proj.weight', 'transformer.layers.3.self_attn.o_proj.weight', 'transformer.layers.3.self_attn.q_proj.bias', 'transformer.layers.3.self_attn.q_proj.weight', 'transformer.layers.3.self_attn.v_proj.bias', 'transformer.layers.3.self_attn.v_proj.weight', 'transformer.layers.4.input_layernorm.weight', 'transformer.layers.4.mlp.down_proj.weight', 'transformer.layers.4.mlp.gate_proj.weight', 'transformer.layers.4.mlp.up_proj.weight', 'transformer.layers.4.post_attention_layernorm.weight', 'transformer.layers.4.self_attn.k_proj.bias', 'transformer.layers.4.self_attn.k_proj.weight', 'transformer.layers.4.self_attn.o_proj.weight', 'transformer.layers.4.self_attn.q_proj.bias', 'transformer.layers.4.self_attn.q_proj.weight', 'transformer.layers.4.self_attn.v_proj.bias', 'transformer.layers.4.self_attn.v_proj.weight', 'transformer.layers.5.input_layernorm.weight', 'transformer.layers.5.mlp.down_proj.weight', 'transformer.layers.5.mlp.gate_proj.weight', 'transformer.layers.5.mlp.up_proj.weight', 'transformer.layers.5.post_attention_layernorm.weight', 'transformer.layers.5.self_attn.k_proj.bias', 'transformer.layers.5.self_attn.k_proj.weight', 'transformer.layers.5.self_attn.o_proj.weight', 'transformer.layers.5.self_attn.q_proj.bias', 'transformer.layers.5.self_attn.q_proj.weight', 'transformer.layers.5.self_attn.v_proj.bias', 'transformer.layers.5.self_attn.v_proj.weight', 'transformer.layers.6.input_layernorm.weight', 'transformer.layers.6.mlp.down_proj.weight', 'transformer.layers.6.mlp.gate_proj.weight', 'transformer.layers.6.mlp.up_proj.weight', 'transformer.layers.6.post_attention_layernorm.weight', 'transformer.layers.6.self_attn.k_proj.bias', 'transformer.layers.6.self_attn.k_proj.weight', 'transformer.layers.6.self_attn.o_proj.weight', 'transformer.layers.6.self_attn.q_proj.bias', 'transformer.layers.6.self_attn.q_proj.weight', 'transformer.layers.6.self_attn.v_proj.bias', 'transformer.layers.6.self_attn.v_proj.weight', 'transformer.layers.7.input_layernorm.weight', 'transformer.layers.7.mlp.down_proj.weight', 'transformer.layers.7.mlp.gate_proj.weight', 'transformer.layers.7.mlp.up_proj.weight', 'transformer.layers.7.post_attention_layernorm.weight', 'transformer.layers.7.self_attn.k_proj.bias', 'transformer.layers.7.self_attn.k_proj.weight', 'transformer.layers.7.self_attn.o_proj.weight', 'transformer.layers.7.self_attn.q_proj.bias', 'transformer.layers.7.self_attn.q_proj.weight', 'transformer.layers.7.self_attn.v_proj.bias', 'transformer.layers.7.self_attn.v_proj.weight', 'transformer.layers.8.input_layernorm.weight', 'transformer.layers.8.mlp.down_proj.weight', 'transformer.layers.8.mlp.gate_proj.weight', 'transformer.layers.8.mlp.up_proj.weight', 'transformer.layers.8.post_attention_layernorm.weight', 'transformer.layers.8.self_attn.k_proj.bias', 'transformer.layers.8.self_attn.k_proj.weight', 'transformer.layers.8.self_attn.o_proj.weight', 'transformer.layers.8.self_attn.q_proj.bias', 'transformer.layers.8.self_attn.q_proj.weight', 'transformer.layers.8.self_attn.v_proj.bias', 'transformer.layers.8.self_attn.v_proj.weight', 'transformer.layers.9.input_layernorm.weight', 'transformer.layers.9.mlp.down_proj.weight', 'transformer.layers.9.mlp.gate_proj.weight', 'transformer.layers.9.mlp.up_proj.weight', 'transformer.layers.9.post_attention_layernorm.weight', 'transformer.layers.9.self_attn.k_proj.bias', 'transformer.layers.9.self_attn.k_proj.weight', 'transformer.layers.9.self_attn.o_proj.weight', 'transformer.layers.9.self_attn.q_proj.bias', 'transformer.layers.9.self_attn.q_proj.weight', 'transformer.layers.9.self_attn.v_proj.bias', 'transformer.layers.9.self_attn.v_proj.weight', 'transformer.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d283aab54094ae3a334e8b8399716ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae81e26b0519408487920ff65c8fed40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace original teacher model initialization\n",
    "teacher_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    teacher_model_name,\n",
    "    trust_remote_code=True  # Required for DeepSeek models\n",
    ")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    teacher_model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False  # Recommended for DeepSeek models\n",
    ")\n",
    "\n",
    "# Modify preprocessing for DeepSeek's tokenization\n",
    "def preprocess_teacher_train(example):\n",
    "    inputs = teacher_tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=512,  # Matches DeepSeek's context window\n",
    "        stride=96,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True  # Explicitly enable special tokens\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26394e5d-3bf9-4b53-8ad5-845763f28a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should do a training pass on SQuAD1.1 to initialise the weights in the Teacher model.\n",
    "# This will also stop the warning from displaying\n",
    "\n",
    "# Should do a training pass on SQuAD1.1 to initialise the weights in the Teacher model.\n",
    "# This will also stop the warning from displaying\n",
    "\n",
    "# New function to preprocess training data for teacher\n",
    "def preprocess_teacher_train(example):\n",
    "    # roberta-base-squad-v1\n",
    "    #inputs = teacher_tokenizer(\n",
    "    #    example[\"question\"],\n",
    "    #    example[\"context\"],\n",
    "    #    truncation=True,\n",
    "    #    max_length=384,\n",
    "    #    stride=128,\n",
    "    #    return_overflowing_tokens=True,\n",
    "    #    return_offsets_mapping=True,\n",
    "    #    padding=\"max_length\"\n",
    "    #)\n",
    "    # DeepSeek-R1-Distill-Qwen-1.5B\n",
    "    inputs = teacher_tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        stride=96,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True  # Explicitly enable special tokens\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = example[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "            \n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "# Teacher training arguments\n",
    "#teacher_training_args = TrainingArguments(\n",
    "#    output_dir=\"./teacher_train\",\n",
    "#    num_train_epochs=3,\n",
    "#    per_device_train_batch_size=16,\n",
    "#    per_device_eval_batch_size=64,\n",
    "#    warmup_steps=500,\n",
    "#    weight_decay=0.01,\n",
    "#    logging_dir=\"./logs\",\n",
    "#    logging_steps=50,\n",
    "#    eval_strategy=\"epoch\",\n",
    "#    save_strategy=\"epoch\",\n",
    "#    load_best_model_at_end=True,\n",
    "#    metric_for_best_model=\"f1\",\n",
    "#    learning_rate=3e-5,\n",
    "#)\n",
    "\n",
    "# Enhanced Teacher Training Arguments\n",
    "teacher_training_args = TrainingArguments(\n",
    "    output_dir=\"./teacher_train\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,  # Increased logging frequency\n",
    "    eval_strategy=\"epoch\",\n",
    "    #eval_steps=500,    # Evaluate every 500 steps\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    learning_rate=3e-5,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=4,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "# Custom progress callback\n",
    "class TeacherTrainingProgress(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"🚀 Starting training with {args.num_train_epochs} epochs\")\n",
    "        print(f\"📊 Batch size: {args.per_device_train_batch_size}\")\n",
    "        print(f\"🔍 Evaluation every {args.eval_steps} steps\")\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"\\n⏳ Starting epoch {state.epoch}/{args.num_train_epochs}\")\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            print(f\"Step {state.global_step}: Loss {logs['loss']:.4f}\")\n",
    "        if logs and 'eval_loss' in logs:\n",
    "            print(f\"Validation Loss: {logs['eval_loss']:.4f}\")\n",
    "            print(f\"Exact Match: {logs['eval_exact_match']:.2f}%\")\n",
    "            print(f\"F1 Score: {logs['eval_f1']:.2f}%\")\n",
    "\n",
    "# Add metrics computation to Trainer\n",
    "def compute_metrics(p):\n",
    "\n",
    "    # Convert logits to predictions\n",
    "    start_pred = np.argmax(p.predictions[0], axis=1)\n",
    "    end_pred = np.argmax(p.predictions[1], axis=1)\n",
    "    \n",
    "    # Get true positions\n",
    "    start_true = p.label_ids[0]\n",
    "    end_true = p.label_ids[1]\n",
    "    \n",
    "    # Calculate exact match\n",
    "    exact_matches = np.logical_and(\n",
    "        start_pred == start_true,\n",
    "        end_pred == end_true\n",
    "    )\n",
    "\n",
    "    # Calculate span F1\n",
    "    def overlap_f1(p_start, p_end, t_start, t_end):\n",
    "        pred_span = set(range(p_start, p_end+1))\n",
    "        true_span = set(range(t_start, t_end+1))\n",
    "        overlap = len(pred_span & true_span)\n",
    "        precision = overlap / len(pred_span) if pred_span else 0\n",
    "        recall = overlap / len(true_span) if true_span else 0\n",
    "        return 2*(precision*recall)/(precision+recall) if (precision+recall) else 0\n",
    "    \n",
    "    f1_scores = [\n",
    "        overlap_f1(sp, ep, st, et)\n",
    "        for sp, ep, st, et in zip(start_pred, end_pred, start_true, end_true)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": np.mean(exact_matches) * 100,\n",
    "        \"f1\": np.mean(f1_scores) * 100\n",
    "    }\n",
    "\n",
    "\n",
    "# Create Trainer for teacher\n",
    "teacher_trainer = Trainer(\n",
    "    model=teacher_model,\n",
    "    args=teacher_training_args,\n",
    "    train_dataset=squad[\"train\"].map(preprocess_teacher_train, batched=True, remove_columns=squad[\"train\"].column_names),\n",
    "    eval_dataset=squad[\"validation\"].map(preprocess_teacher_train, batched=True, remove_columns=squad[\"validation\"].column_names),\n",
    "    tokenizer=teacher_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[TeacherTrainingProgress()]\n",
    ")\n",
    "\n",
    "# Train teacher model\n",
    "print(\"\\nTraining Teacher Model on SQuAD1.1...\")\n",
    "teacher_trainer.train()\n",
    "teacher_model.save_pretrained(\"./trained_teacher\")\n",
    "teacher_tokenizer.save_pretrained(\"./trained_teacher\")\n",
    "\n",
    "print(\"\\nRe-loading optimized teacher model\")\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(\"./trained_teacher\")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(\"./trained_teacher\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315d26ea-291f-4970-af2d-7be96a264980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load student model and tokenizer (smaller version of RoBERTa)\n",
    "student_model_name = \"distilroberta-base\"  # Example smaller model\n",
    "student_model = AutoModelForQuestionAnswering.from_pretrained(student_model_name)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de9a5d3-c578-4120-af5e-b0ee3bf4a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.to(device)\n",
    "    \n",
    "    metric = evaluate.load(\"squad\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Evaluating\"):\n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer(\n",
    "            example[\"context\"], example[\"question\"], truncation=True, padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = model(**inputs)\n",
    "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "        start_idx = torch.argmax(start_logits, dim=-1).item()\n",
    "        end_idx = torch.argmax(end_logits, dim=-1).item()\n",
    "        \n",
    "        # Decode prediction\n",
    "        prediction = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx + 1])\n",
    "        \n",
    "        # Append to predictions\n",
    "        predictions.append({\n",
    "            \"id\": example[\"id\"],\n",
    "            \"prediction_text\": prediction\n",
    "        })\n",
    "\n",
    "        # Append to references (ground truth)\n",
    "        references.append({\n",
    "            \"id\": example[\"id\"],\n",
    "            \"answers\": example[\"answers\"]\n",
    "        })\n",
    "\n",
    "    # Compute metrics\n",
    "    result = metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"Exact Match: {result['exact_match']:.2f}%\")\n",
    "    print(f\"F1 Score: {result['f1']:.2f}%\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d7aebd-b439-4eee-b31f-4d27cd5f75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_data(example):\n",
    "    # Tokenize context and question\n",
    "    inputs = student_tokenizer(\n",
    "        example[\"context\"],\n",
    "        example[\"question\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=384,\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c99cdd0b-7def-4d6a-89c4-7710dd4ddc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Model Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 221/10570 [00:09<07:47, 22.15it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.59 GiB of which 10.31 MiB is free. Process 1553615 has 5.44 GiB memory in use. Process 1654911 has 10.13 GiB memory in use. Of the allocated memory 9.50 GiB is allocated by PyTorch, and 353.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#if debugging: \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#    validation_set = validation_dataset.select(range(5000))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#    validation_dataset = validation_set\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeacher Model Evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate student model on validation set (before distillation)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudent Model Evaluation (Before Distillation)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, dataset)\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Get model outputs\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m start_logits, end_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mstart_logits, outputs\u001b[38;5;241m.\u001b[39mend_logits\n\u001b[1;32m     21\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(start_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:1087\u001b[0m, in \u001b[0;36mQwen2ForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(QWEN2_INPUTS_DOCSTRING)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1075\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuestionAnsweringModelOutput:\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;124;03m    start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;124;03m        Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03m        are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1087\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1099\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:549\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    538\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    539\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m         position_embeddings,\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:278\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    277\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 278\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    281\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:59\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 59\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.59 GiB of which 10.31 MiB is free. Process 1553615 has 5.44 GiB memory in use. Process 1654911 has 10.13 GiB memory in use. Of the allocated memory 9.50 GiB is allocated by PyTorch, and 353.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Evaluate teacher model on validation set\n",
    "\n",
    "validation_dataset = squad[\"validation\"]\n",
    "\n",
    "#if debugging: \n",
    "#    validation_set = validation_dataset.select(range(5000))\n",
    "#    validation_dataset = validation_set\n",
    "\n",
    "print(\"Teacher Model Evaluation\")\n",
    "result = evaluate_model(teacher_model, teacher_tokenizer, validation_dataset)\n",
    "\n",
    "# Evaluate student model on validation set (before distillation)\n",
    "print(\"Student Model Evaluation (Before Distillation)\")\n",
    "result = evaluate_model(student_model, student_tokenizer, validation_dataset)\n",
    "\n",
    "# Save the results for use in Graphs\n",
    "exact_match_before = result['exact_match']\n",
    "f1_score_before = result['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857eec0-4fd0-489f-82bf-8963125fbfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for distillation\n",
    "def preprocess_data(example):\n",
    "\n",
    "    # Move teacher model to appropriate device\n",
    "    teacher_model.to(device)\n",
    "    \n",
    "    # Tokenize context and question\n",
    "    inputs = teacher_tokenizer(\n",
    "        example[\"context\"], \n",
    "        example[\"question\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=384,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Move inputs to the same device as the teacher model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Get logits from the teacher model\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(**inputs)\n",
    "    \n",
    "    # Add teacher logits to the example\n",
    "    example[\"input_ids\"] = inputs[\"input_ids\"][0].cpu().tolist()\n",
    "    example[\"attention_mask\"] = inputs[\"attention_mask\"][0].cpu().tolist()\n",
    "    example[\"start_logits\"] = outputs.start_logits[0].cpu().tolist()\n",
    "    example[\"end_logits\"] = outputs.end_logits[0].cpu().tolist()\n",
    "    \n",
    "    return example\n",
    "\n",
    "if debugging: \n",
    "    train_dataset = squad[\"train\"].select(range(30000)).map(preprocess_data)\n",
    "else:\n",
    "    train_dataset = squad[\"train\"].map(preprocess_data)    \n",
    "\n",
    "# Apply preprocessing to validation dataset\n",
    "validation_set = validation_dataset.map(preprocess_validation_data, batched=True)\n",
    "\n",
    "# Define training arguments for student model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=10,          # Log every 10 steps need when using small datasets for testing\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406a9b4-d143-4879-8451-cf27c3fdb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss function for knowledge distillation\n",
    "class DistillationTrainer(Trainer):    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom loss function for knowledge distillation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        \n",
    "        # Compute KL Divergence loss between teacher and student logits\n",
    "        start_loss = torch.nn.functional.kl_div(\n",
    "            torch.nn.functional.log_softmax(outputs.start_logits, dim=-1),\n",
    "            torch.nn.functional.softmax(inputs[\"start_logits\"], dim=-1),\n",
    "            reduction=\"batchmean\"\n",
    "        )\n",
    "        end_loss = torch.nn.functional.kl_div(\n",
    "            torch.nn.functional.log_softmax(outputs.end_logits, dim=-1),\n",
    "            torch.nn.functional.softmax(inputs[\"end_logits\"], dim=-1),\n",
    "            reduction=\"batchmean\"\n",
    "        )\n",
    "        \n",
    "        # Average the start and end losses\n",
    "        loss = (start_loss + end_loss) / 2\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a49a4-a83b-4f49-a9a5-718fb9e9d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyCustomDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Filter out non-tokenized fields before padding\n",
    "        tokenized_features = [\n",
    "            {k: v for k, v in f.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "            for f in features\n",
    "        ]\n",
    "\n",
    "        # Dynamically pad input_ids and attention_mask using the tokenizer\n",
    "        batch = self.tokenizer.pad(\n",
    "            tokenized_features,\n",
    "            padding=True,\n",
    "            max_length=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Add custom fields (e.g., start_logits and end_logits) to the batch\n",
    "        if \"start_logits\" in features[0]:\n",
    "            batch[\"start_logits\"] = torch.tensor([f[\"start_logits\"] for f in features], dtype=torch.float32)\n",
    "        if \"end_logits\" in features[0]:\n",
    "            batch[\"end_logits\"] = torch.tensor([f[\"end_logits\"] for f in features], dtype=torch.float32)\n",
    "\n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f88f2f-979d-477b-8df2-049e31e1f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = FullyCustomDataCollator(tokenizer=student_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891ee9b-3f3b-4f9c-b035-b37ccade6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the student model using the custom trainer\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_set,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "#transformers_logger.setLevel(logging.INFO)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58311f7e-cd7f-4e43-9b53-42222f0b4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate student model on validation set after distillation\n",
    "print(\"Student Model Evaluation after Distillation\")\n",
    "result = evaluate_model(student_model, student_tokenizer, validation_dataset)\n",
    "\n",
    "# Save the results for use in Graphs\n",
    "exact_match_after = result['exact_match']\n",
    "f1_score_after = result['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7541c-e517-413f-80bc-136cd1bcef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss values per epoch (replace with actual values extracted from `trainer.state.log_history`)\n",
    "# Generate 50 values starting at 0.8 and decaying exponentially to 0.19\n",
    "factor = (0.19 / 0.8) ** (1 / 49)\n",
    "loss_values = [0.19 * (factor ** i) for i in range(50)]\n",
    "\n",
    "# Plotting loss values per epoch\n",
    "plt.figure(figsize=(8, 6))\n",
    "epochs = np.arange(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, marker='o', label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting Exact Match scores before and after distillation\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = [\"Before Distillation\", \"After Distillation\"]\n",
    "exact_match_scores = [exact_match_before, exact_match_after]\n",
    "plt.bar(labels, exact_match_scores, color=['blue', 'green'])\n",
    "plt.ylabel(\"Exact Match (%)\")\n",
    "plt.title(\"Exact Match Score Before and After Distillation\")\n",
    "for i, v in enumerate(exact_match_scores):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Plotting F1 scores before and after distillation\n",
    "plt.figure(figsize=(8, 6))\n",
    "f1_scores = [f1_score_before, f1_score_after]\n",
    "plt.bar(labels, f1_scores, color=['orange', 'red'])\n",
    "plt.ylabel(\"F1 Score (%)\")\n",
    "plt.title(\"F1 Score Before and After Distillation\")\n",
    "for i, v in enumerate(f1_scores):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3302d-1b61-4310-af12-4c79f13584cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
